<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script type="text/javascript" async="" src="https://www.googletagmanager.com/gtag/js?id=G-3SWPHB3SK1&amp;l=dataLayer&amp;cx=c"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-177517745-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-177517745-1");
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Home | Jiaxu Wu</title>
    <meta name="description" content="Jiaxu Wu is a doctor student at the Department of Precision Engineering, the University of Tokyo. His research interests include robot navigation, human motion prediction, deep learning, and learn-based control. Feel free to connect!">
    <meta name="keywords" content="research, mobile robot navigation, deep learning, reinforcement learning">
    <meta name="author" content="Jiaxu Wu">

    <!-- Font Awesome if you need it
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css">
  -->
    <link rel="stylesheet" href="/build/tailwind.css">

    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">

    <!-- Define your gradient here - use online tools to find a gradient matching your branding-->
    <style>
      .gradient {
        background: linear-gradient(90deg, #8546fa 0%, #17d851 100%);
      }

      li {
        margin: 0.25rem 0;
      }
    </style>
</head>
<body class="leading-normal tracking-normal text-white gradient" style="font-family: 'Source Sans Pro', sans-serif;" data-new-gr-c-s-check-loaded="14.1114.0" data-gr-ext-installed="">
    <!--Nav-->
    <nav id="header" class="fixed w-full z-30 top-0 text-white">
      <div class="w-full container mx-auto flex flex-wrap items-center justify-between mt-0 py-2">
        <div class="pl-4 flex items-center text-white toggleColor text-lg">
          J. Wu
        </div>

        <div class="block lg:hidden pr-4">
          <button id="nav-toggle" class="flex items-center p-1 text-orange-800 hover:text-gray-900">
            <svg class="fill-current h-6 w-6" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
              <title>Menu</title>
              <path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z"></path>
            </svg>
          </button>
        </div>
        <div class="w-full flex-grow lg:flex lg:items-center lg:w-auto lg:block mt-2 lg:mt-0 bg-white lg:bg-transparent text-black p-4 lg:p-0 z-20 hidden" id="nav-content">
          <ul class="list-reset lg:flex justify-end flex-1 items-center">
            <li class="mr-3">
              <a class="inline-block font-bold text-white no-underline hover:text-indigo-800 py-2 px-4 toggleColor" href="#">
                Home
              </a>
            </li>
            <!-- <li class="mr-3">
              <a
                class="inline-block text-black no-underline hover:text-indigo-800 py-2 px-4"
                href="/publications.html"
                >Publications</a
              >
            </li> -->
          </ul>
        </div>
      </div>
      <hr class="border-b border-gray-100 opacity-25 my-0 py-0">
    </nav>

    <!--Profile-->
    <div class="pt-16">
      <div class="container px-3 mx-auto flex flex-wrap flex-col md:flex-row items-center">
        <!--Left Col-->
        <div class="w-full md:w-2/5 py-4 px-2 text-center">
          <img class="w-48 h-48 md:w-64 md:h-64 mx-auto rounded-full" src="img/profile.jpg">
          <h2 class="leading-normal text-2xl my-1">Jiaxu Wu</h2>
          <!-- <p class="my-2">
            <a
              href="https://github.com/matsuren"
              target="_blank"
              rel="noopener noreferrer"
              class="underline"
              >GitHub</a
            >,
            <a
              href="https://scholar.google.com/citations?user=xlY6tG8AAAAJ"
              target="_blank"
              rel="noopener noreferrer"
              class="underline"
              >Google Scholar</a
            >,
            <a
              href="https://www.researchgate.net/profile/Ren_Komatsu"
              target="_blank"
              rel="noopener noreferrer"
              class="underline"
              >ResearchGate</a
            >
          </p> -->
          <!-- <img class="w-56 md:w-64 mx-auto my-1" src="img/contact.png" alt="contact"> -->
          <td class="border px-4 py-2">
            wujiaxu@robot.t.u-tokyo.ac.jp
          </td>
        </div>
        <!--Right Col-->
        <div class="flex flex-col w-full md:w-3/5 px-2 justify-center items-start text-left">
          <h1 class="my-4 text-2xl md:text-3xl font-bold leading-tight">
            About me
          </h1>
          <h2 class="leading-normal text-lg md:text-xl mb-2">
            I am a doctor student at the Department of Precision
            Engineering, the University of Tokyo. I finished my Master degree
            June 2020 at the same university, where I was advised by
            <a href="http://www.robot.t.u-tokyo.ac.jp/~yamashita/" target="_blank" rel="noopener noreferrer" class="underline">
              Atsushi Yamashita
            </a>
            and
            <a href="http://www.robot.t.u-tokyo.ac.jp/asamalab/Asama_page/top_a.html" target="_blank" rel="noopener noreferrer" class="underline">
              Hajime Asama
            </a>
            .
          </h2>
          <h2 class="leading-normal text-lg md:text-xl mb-8">
            After Master degree I worked in Yaskawa Electric Corporation for 2 years as a robotics engineer.
            I joined pin-picking project 
            <a href="https://www.youtube.com/watch?v=LOtVgzNKiAs" target="_blank" rel="noopener noreferrer" class="underline">
                MotoSight AI Picking
            </a>
            which guides the industrial robot picking by RGBD vision and sim2real AI.
          </h2>
          <h2 class="leading-normal text-lg md:text-xl mb-8">
            I backed to UTokyo in Apr. 2022 and started research on reinforcement 
            learning-based robot navigation in human populated environment.
            My research interests include robot navigation, human motion prediction, 
            deep learning, and learn-based control. Feel free to connect!
          </h2>
        </div>
      </div>
    </div>

    <div class="relative -mt-12 lg:-mt-24">
      <svg viewBox="0 0 1428 174" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="pointer-events: none;">
        <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
          <g transform="translate(-2.000000, 44.000000)" fill="#FFFFFF" fill-rule="nonzero">
            <path d="M0,0 C90.7283404,0.927527913 147.912752,27.187927 291.910178,59.9119003 C387.908462,81.7278826 543.605069,89.334785 759,82.7326078 C469.336065,156.254352 216.336065,153.6679 0,74.9732496" opacity="0.100000001"></path>
            <path d="M100,104.708498 C277.413333,72.2345949 426.147877,52.5246657 546.203633,45.5787101 C666.259389,38.6327546 810.524845,41.7979068 979,55.0741668 C931.069965,56.122511 810.303266,74.8455141 616.699903,111.243176 C423.096539,147.640838 250.863238,145.462612 100,104.708498 Z" opacity="0.100000001"></path>
            <path d="M1046,51.6521276 C1130.83045,29.328812 1279.08318,17.607883 1439,40.1656806 L1439,120 C1271.17211,77.9435312 1140.17211,55.1609071 1046,51.6521276 Z" id="Path-4" opacity="0.200000003"></path>
          </g>
          <g transform="translate(-4.000000, 76.000000)" fill="#FFFFFF" fill-rule="nonzero">
            <path d="M0.457,34.035 C57.086,53.198 98.208,65.809 123.822,71.865 C181.454,85.495 234.295,90.29 272.033,93.459 C311.355,96.759 396.635,95.801 461.025,91.663 C486.76,90.01 518.727,86.372 556.926,80.752 C595.747,74.596 622.372,70.008 636.799,66.991 C663.913,61.324 712.501,49.503 727.605,46.128 C780.47,34.317 818.839,22.532 856.324,15.904 C922.689,4.169 955.676,2.522 1011.185,0.432 C1060.705,1.477 1097.39,3.129 1121.236,5.387 C1161.703,9.219 1208.621,17.821 1235.4,22.304 C1285.855,30.748 1354.351,47.432 1440.886,72.354 L1441.191,104.352 L1.121,104.031 L0.457,34.035 Z"></path>
          </g>
        </g>
      </svg>
    </div>

    <!-- Experiences -->
    <section class="bg-white border-b py-8 text-black">
      <div class="container max-w-4xl mx-auto m-7 px-2">
        <h1 class="w-full my-2 text-4xl font-bold leading-tight text-center text-gray-800">
          Experience
        </h1>
        <div class="w-full mb-4">
          <div class="h-1 mx-auto gradient w-64 opacity-25 my-0 py-0 rounded-t"></div>
        </div>
        <table class="table-auto my-2">
          <tbody>
            <tr class="bg-gray-100">
              <td class="border px-4 py-2">Apr. 2022 - present</td>
              <td class="border px-4 py-2">
                Doctor degree, the Department of Precision
                Engineering, the University of Tokyo.
              </td>
            </tr>
            <tr>
              <td class="border px-4 py-2">Apr. 2020 - Mar. 2022</td>
              <td class="border px-4 py-2">
                Robotics engineer, Vision Group of the Autonomous 
                Technology Development Section, Yaskawa Electric Corporation.
              </td>
            </tr>
            <tr class="bg-gray-100">
              <td class="border px-4 py-2">Apr. 2018 - Mar. 2020</td>
              <td class="border px-4 py-2">
                Master degree, the Department of Precision
                Engineering, the University of Tokyo.
              </td>
            </tr>
          </tbody>
        </table>
        <!-- <div class="m-2">
          Curriculum Vitae is available
          <a class="text-purple-800" href="https://docs.google.com/document/d/1w6csWrHToGvulIEXoLqsda6rxwoDNrrMbhohULy7mN8/edit?usp=sharing" target="_blank" rel="noopener noreferrer">
            here [CV]
          </a>
          .
        </div> -->
      </div>
    </section>

    <!-- Selected Papers -->
    <section class="bg-white border-b py-8">
      <div class="container mx-auto flex flex-wrap pt-4 pb-12">
        <h1 class="w-full my-2 text-4xl font-bold leading-tight text-center text-gray-800">
          Selected Papers
        </h1>
        <div class="w-full mb-4">
          <div class="h-1 mx-auto gradient w-64 opacity-25 my-0 py-0 rounded-t"></div>
        </div>

        <div class="w-full md:w-1/2 p-4 flex flex-col flex-grow flex-shrink">
          <div class="flex-1 bg-white rounded-t rounded-b-none overflow-hidden shadow">
            <div class="flex flex-wrap no-underline hover:no-underline">
              <p class="w-full text-gray-600 text-xs md:text-sm px-6">
                CONFERENCE: IROS 2023
              </p>
              <div class="w-full font-bold text-xl text-gray-800 px-6">
                Risk-Sensitive Mobile Robot Navigation in Crowded Environment 
                via Offline Reinforcement Learning
              </div>
              <p class="text-gray-800 text-base px-6">
                To realize risk sensitivity and improve the safety of the offline RL agent during deployment, 
                this work proposes a multipolicy control framework that combines offline RL navigation policy 
                with a risk detector and a force-based risk-avoiding policy. In particular, a Lyapunov density 
                model is learned using the latent feature of the offline RL policy and works as a risk detector 
                to switch the control to the risk-avoiding policy when the robot has a tendency to go out of 
                the area supported by the training data. Experimental results showed that the proposed method 
                was able to learn navigation in a crowded scene from the offline trajectory dataset and the risk 
                detector substantially reduces the collision rate of the vanilla offline RL agent while maintaining 
                the navigation efficiency outperforming the state-of-the-art methods. 
              </p>
              <p class="text-base px-6 mb-5 text-purple-800">
                <!-- <a href="https://arxiv.org/abs/2007.06891" target="_blank" rel="noopener noreferrer">
                  [arXiv]
                </a>
                <a href="https://github.com/matsuren/crownconv360depth" target="_blank" rel="noopener noreferrer">
                  [code]
                </a> -->

                <a href="https://drive.google.com/file/d/1ielixG6P61yED9ENcR529Nj4oGf1NMJs/view?usp=sharing" target="_blank" rel="noopener noreferrer">
                  [video]
                </a>
              </p>
            </div>
          </div>
        </div>

        <div class="w-full md:w-1/2 p-4 flex flex-col flex-grow flex-shrink">
          <div class="flex-1 bg-white rounded-t rounded-b-none overflow-hidden shadow">
            <div class="flex flex-wrap no-underline hover:no-underline">
              <p class="w-full text-gray-600 text-xs md:text-sm px-6">
                CONFERENCE: IAS  2023
              </p>
              <div class="w-full font-bold text-xl text-gray-800 px-6">
                Give Pedestrian More Choice: Socially Aware Navigation 
                Using Reinforcement Learning with Human Action Entropy Maximization
              </div>
              <p class="text-gray-800 text-base px-6">
                In this paper, we propose a socially aware navigation method using
                reinforcement learning with human action entropy maximization, in which the
                diversity of human choices is considered as a reward to inform the robot of the
                socially acceptable interaction with a crowd. By learning to maximize this reward,
                the proposed method realizes social acceptability by giving pedestrians
                more choices to enlarge their chance to take their preferred action.
              </p>
              <p class="text-base px-6 mb-5 text-purple-800">
                <a href="https://ias-18.org/wp-content/uploads/2023/05/0044_FI.pdf" target="_blank" rel="noopener noreferrer">
                  [paper]
                </a>
                <!-- <a href="https://github.com/matsuren/octDPSNet" target="_blank" rel="noopener noreferrer">
                  [code]
                </a> -->
                <a href="https://drive.google.com/file/d/1nYEZg7lQnFZUsZTWgKarwtDKeguUkZv-/view?usp=sharing" target="_blank" rel="noopener noreferrer">
                  [slide]
                </a>
              </p>
            </div>
          </div>
        </div>

        <div class="w-full md:w-1/2 p-4 flex flex-col flex-grow flex-shrink">
          <div class="flex-1 bg-white rounded-t rounded-b-none overflow-hidden shadow">
            <div class="flex flex-wrap no-underline hover:no-underline">
              <p class="w-full text-gray-600 text-xs md:text-sm px-6">
                JOURNAL: IEEE Robotics and Automation Letters 2020
              </p>
              <div class="w-full font-bold text-xl text-gray-800 px-6">
                Smartphone Zombie Detection from LiDAR Point Cloud for Mobile Robot Safety
              </div>
              <p class="text-gray-800 text-base px-6">
                Awareness of surrounding and prediction of dangerous situations is essential for autonomous mobile robots, 
                especially during navigation in a human-populated environment. 
                To cope with safety issues, state-of-the-art works have focused on 
                pedestrian detection, tracking, and trajectory prediction. However, 
                only a few studies have been conducted on recognizing some specific types of dangerous 
                behaviors exhibited by pedestrians. 
                Here, we propose a tracking enhanced detection method to recognize 
                people using their smartphones while walking, referred to as smartphone zombie. 
                Features used for pedestrian detection usually involve the rotation variance problem, 
                and in this paper, the drawback is handled by employing motion information from multi-object tracking. 
                The proposed solution has been validated through experiments performed on a newly collected dataset. 
                Results showed that our detector can learn a distinct pattern of the appearance of smartphone zombies. 
                Thus, it can successfully detect them outperforming the existed detection method.
              </p>
              <p class="text-base px-6 mb-5 text-purple-800">
                <a href="https://ieeexplore.ieee.org/document/8976312" target="_blank" rel="noopener noreferrer">
                  [paper]
                </a>
                <!-- <a href="https://github.com/matsuren/fvp_viewer" target="_blank" rel="noopener noreferrer">
                  [code]
                </a> -->
                <a href="http://www.robot.t.u-tokyo.ac.jp/~yamashita/paper/A/A139.mp4" target="_blank" rel="noopener noreferrer">
                  [video]
                </a>
                <!-- <a
                  href="https://youtu.be/4ptVopsKt9A"
                  target="_blank"
                  rel="noopener noreferrer"
                  >[video]</a
                > -->
              </p>
            </div>
          </div>
        </div>

        <div class="w-full md:w-1/2 p-4 flex flex-col flex-grow flex-shrink">
          <div class="flex-1 bg-white rounded-t rounded-b-none overflow-hidden shadow">
            <div class="flex flex-wrap no-underline hover:no-underline">
              <p class="w-full text-gray-600 text-xs md:text-sm px-6">-</p>
              <div class="w-full font-bold text-xl text-gray-800 px-6"></div>
              <p class="text-gray-800 text-base px-6 mb-5"></p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- template -->
    <!-- <section class="bg-white border-b py-8">
      <div class="container max-w-4xl mx-auto m-7">
        <h1
          class="w-full my-2 text-4xl font-bold leading-tight text-center text-gray-800"
        >
          Publications
        </h1>
        <div class="w-full mb-4">
          <div
            class="h-1 mx-auto gradient w-64 opacity-25 my-0 py-0 rounded-t"
          ></div>
        </div>
        <div class="text-black text-4xl">Underconstruction</div>
      </div>
    </section> -->

    <!-- Publications -->
    <!-- <section class="bg-white border-b pt-8">
      <div class="container max-w-4xl mx-auto m-7">
        <h1 class="w-full my-2 text-4xl font-bold leading-tight text-center text-gray-800">
          Publications
        </h1>
        <div class="w-full mb-4">
          <div class="h-1 mx-auto gradient w-64 opacity-25 my-0 py-0 rounded-t"></div>
        </div>
        <div class="text-black p-4 m-2">
          <div>
            <em>
              *Japanese publications are not included here. Please visit
              <a href="https://researchmap.jp/kren/published_papers" class="underline text-purple-800" target="_blank" rel="noopener noreferrer">
                researchmap
              </a>
              or
              <a class="underline text-purple-800" href="https://docs.google.com/document/d/1w6csWrHToGvulIEXoLqsda6rxwoDNrrMbhohULy7mN8/edit?usp=sharing" target="_blank" rel="noopener noreferrer">
                CV
              </a>
              for my complete publications.
            </em>
          </div>
          <ol class="list-decimal">
            <li>
              Wanxin Bao,
              <strong>Ren Komatsu</strong>
              , Atsushi Yamashita, and Hajime Asama, “Solving Well-posed Shape
              from Shading Problem Using Implicit Neural Representations”,
              <em>
                Proceedings of the 2nd International Conference on Image
                Processing and Robotics
              </em>
              , 2022.
            </li>

            <li>
              Ziheng Chao,
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Yusuke Tamura, Atsushi Yamashita, and Hajime Asama,
              “Radiation Distribution Estimation with Non-directional Detector
              Using Plane Source Model”,
              <em>Advanced Robotics</em>
              , vol. 36, no. 4, pp. 182-191, 2022.
            </li>

            <li>
              Haoxiang Liu,
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Yusuke Tamura, Atsushi Yamashita, and Hajime Asama,
              “Viewpoint Selection without Subject Experiments for Teleoperation
              of Robot Arm in Reaching Task Using Reinforcement Learning”,
              <em>
                Proceedings of the 2022 IEEE/SICE International Symposium on
                System Integration
              </em>
              , pp. 1015-1020, 2022.
            </li>
            <li>
              Ziheng Chao,
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Yusuke Tamura, Atsushi Yamashita, and Hajime Asama,
              “Estimation of Radiation Source Distribution Using Structure
              Information for Fukushima Daiichi Nuclear Power Plant Reactor”,
              <em>
                Proceedings of the 2022 IEEE/SICE International Symposium on
                System Integration
              </em>
              , pp. 1030-1035, 2022.
            </li>
            <li>
              Yuta Sugasawa, Shota Chikushi,
              <strong>Ren Komatsu</strong>
              , Jun Younes Louhi Kasahara, Sarthak Pathak, Ryosuke Yajima,
              Shunsuke Hamasaki, Keiji Nagatani, Takumi Chiba, Kazuhiro Chayama,
              Atsushi Yamashita, and Hajime Asama, “Visualization of Dump Truck
              and Excavator in Bird's-eye View by Fisheye Cameras and 3D Range
              Sensor”,
              <em>Proceedings of the 16th International Conference IAS-16</em>
              , pp. 480-491, 2021.
            </li>
            <li>
              Hao Xu,
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Atsushi Yamashita, and Hajime Asama, "Leakage
              Position Estimation of Cooling Water Using a Stereo Camera for
              Fukushima Daiichi Nuclear Power Plant",
              <em>Applied Sciences</em>
              , vol. 11, no. 17, 7796, pp. 1-15, 2021.
            </li>
            <li>
              Takuya Kishimoto, Hanwool Woo,
              <strong>Ren Komatsu</strong>
              , Yusuke Tamura, Hideki Tomita, Kenji Shimazoe, Atsushi Yamashita,
              and Hajime Asama, "Path Planning for Localization of Radiation
              Sources Based on Principal Component Analysis",
              <em>Applied Sciences</em>
              , vol. 11, no. 10, 4707, pp. 1-22, 2021.
            </li>
            <li>
              Runqiu Bao,
              <strong>Ren Komatsu</strong>
              , Renato Miyagusuku, Masaki Chino, Atsushi Yamashita, and Hajime
              Asama, "Stereo camera visual SLAM with hierarchical masking and
              motion-state classification at outdoor construction sites
              containing large dynamic objects",
              <em>Advanced Robotics</em>
              , vol. 35, no. 3-4, pp. 228-241, 2021.
            </li>
            <li>
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Yusuke Tamura, Atsushi Yamashita, and Hajime Asama,
              "Gamma-ray Image Noise Generation Using Energy-Image Converter
              Based on Image Histogram",
              <em>
                Proceedings of the 2021 IEEE/SICE International Symposium on
                System Integration (SII2021)
              </em>
              , 2021.
            </li>
            <li>
              Hao Xu,
              <strong>Ren Komatsu</strong>
              , Hanwool Woo, Angela Faragasso, Atsushi Yamashita, and Hajime
              Asama, "Camera Orientation Estimation in Leaking Indoor
              Environment via Vanishing Point of Water Drops",
              <em>
                Proceedings of the 2020 IEEE International Symposium on Safety,
                Security, and Rescue Robotics (SSRR2020)
              </em>
              , 2020.
            </li>
            <li>
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Yusuke Tamura, Atsushi Yamashita, and Hajime
              Asama, "360° Depth Estimation from Multiple Fisheye Images
              with Origami Crown Representation of Icosahedron",
              <em>
                Proceedings of the 2020 IEEE/RSJ International Conference on
                Intelligent Robots and Systems (IROS2020)
              </em>
              , 2020.
            </li>
            <li>
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Yusuke Tamura, Atsushi Yamashita, and Hajime
              Asama, "Free Viewpoint Image Generation System Using Fisheye
              Cameras and a Laser Rangefinder for Indoor Robot Teleoperation",
              <em>ROBOMECH Journal</em>
              , vol. 7, 15, pp.1-10, 2020.
            </li>
            <li>
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Yusuke Tamura, Atsushi Yamashita, and Hajime
              Asama, "Octave Deep Plane-sweeping Network: Reducing Spatial
              Redundancy for Learning-based Plane-sweeping Stereo",
              <em>IEEE Access</em>
              , vol. 7, pp. 150306-150317, 2019.
            </li>
            <li>
              Runqiu Bao,
              <strong>Ren Komatsu</strong>
              , Renato Miyagusuku, Masaki Chino, Atsushi Yamashita, and Hajime
              Asama, "Cost-effective and Robust Visual Based Localization with
              Consumer-level Cameras at Construction Sites",
              <em>
                Proceedings of the 2019 IEEE 8th Global Conference on Consumer
                Electronics (GCCE2019)
              </em>
              , pp. 1007-1009, Osaka, Japan, 2019.
            </li>
            <li>
              Dabae Kim, Sarthak Pathak, Alessandro Moro,
              <strong>Ren Komatsu</strong>
              , Atsushi Yamashita and Hajime Asama, "E-CNN: Accurate Spherical
              Camera Rotation Estimation via Uniformization of Distorted Optical
              Flow Fields",
              <em>
                Proceedings of the 2019 IEEE International Conference on
                Acoustics, Speech, and Signal Processing (ICASSP2019),
              </em>
              Brighton, UK, 2019.
            </li>
            <li>
              Yasuyuki Awashima,
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Yusuke Tamura, Atsushi Yamashita, and Hajime
              Asama, “Visualization of Obstacles on Bird’s-eye View
              Using Depth Sensor for Remote Controlled Robot”,
              <em>
                Proceedings of the International Workshop on Advanced Image
                Technology 2017 (IWAIT2017)
              </em>
              , Penang, Malaysia, 2017.
            </li>
            <li>
              Wei Sun, Soichiro Iwataki,
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Atsushi Yamashita, and Hajime Asama,
              “Simultaneous Tele-visualization of Construction Machine and
              Environment Using Body Mounted Cameras”,
              <em>
                Proceedings of the 2016 IEEE International Conference on
                Robotics and Biomimetics (ROBIO2016)
              </em>
              , pp.382-387, Qingdao, China, 2016.
            </li>
            <li>
              <strong>Ren Komatsu</strong>
              , Hiromitsu Fujii, Hitoshi Kono, Yusuke Tamura, Atsushi Yamashita,
              and Hajime Asama, “Bird's-eye View Image Generation with
              Camera Malfunction in Irradiation Environment”,
              <em>
                Proceedings of the 6th International Conference on Advanced
                Mechatronics (ICAM2015)
              </em>
              , pp. 177-178, Tokyo, Japan, 2015.
            </li>
          </ol>
        </div>
      </div>
    </section> -->

    <!-- Change the Color #f8fafc to match the previous section Color -->
    <svg class="wave-top" viewBox="0 0 1439 147" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g transform="translate(-1.000000, -14.000000)" fill-rule="nonzero">
          <g class="wave" fill="#f8fafc">
            <path d="M1440,84 C1383.555,64.3 1342.555,51.3 1317,45 C1259.5,30.824 1206.707,25.526 1169,22 C1129.711,18.326 1044.426,18.475 980,22 C954.25,23.409 922.25,26.742 884,32 C845.122,37.787 818.455,42.121 804,45 C776.833,50.41 728.136,61.77 713,65 C660.023,76.309 621.544,87.729 584,94 C517.525,105.104 484.525,106.438 429,108 C379.49,106.484 342.823,104.484 319,102 C278.571,97.783 231.737,88.736 205,84 C154.629,75.076 86.296,57.743 0,32 L0,0 L1440,0 L1440,84 Z"></path>
          </g>
          <g transform="translate(1.000000, 15.000000)" fill="#FFFFFF">
            <g transform="translate(719.500000, 68.500000) rotate(-180.000000) translate(-719.500000, -68.500000) ">
              <path d="M0,0 C90.7283404,0.927527913 147.912752,27.187927 291.910178,59.9119003 C387.908462,81.7278826 543.605069,89.334785 759,82.7326078 C469.336065,156.254352 216.336065,153.6679 0,74.9732496" opacity="0.100000001"></path>
              <path d="M100,104.708498 C277.413333,72.2345949 426.147877,52.5246657 546.203633,45.5787101 C666.259389,38.6327546 810.524845,41.7979068 979,55.0741668 C931.069965,56.122511 810.303266,74.8455141 616.699903,111.243176 C423.096539,147.640838 250.863238,145.462612 100,104.708498 Z" opacity="0.100000001"></path>
              <path d="M1046,51.6521276 C1130.83045,29.328812 1279.08318,17.607883 1439,40.1656806 L1439,120 C1271.17211,77.9435312 1140.17211,55.1609071 1046,51.6521276 Z" opacity="0.200000003"></path>
            </g>
          </g>
        </g>
      </g>
    </svg>

    <!--Footer-->
    <footer class="gradient p-1">
      <div class="container mx-auto text-white">
        <div class="text-lg py-1 text-center">© 2023 Jiaxu Wu</div>
      </div>
    </footer>

    <!-- jQuery if you need it
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  -->

    <script>
      var scrollpos = window.scrollY;
      var header = document.getElementById("header");
      var navcontent = document.getElementById("nav-content");
      var toToggle = document.querySelectorAll(".toggleColor");

      document.addEventListener("scroll", function () {
        /*Apply classes for slide in bar*/
        scrollpos = window.scrollY;

        if (scrollpos > 20) {
          header.classList.add("bg-white");
          //Use to switch toggleColor Colors
          for (var i = 0; i < toToggle.length; i++) {
            toToggle[i].classList.add("text-black");
            toToggle[i].classList.remove("text-white");
          }
          header.classList.add("shadow");
          navcontent.classList.remove("bg-gray-100");
          navcontent.classList.add("bg-white");
        } else {
          header.classList.remove("bg-white");
          //Use to switch toggleColor Colors
          for (var i = 0; i < toToggle.length; i++) {
            toToggle[i].classList.add("text-white");
            toToggle[i].classList.remove("text-black");
          }

          header.classList.remove("shadow");
          navcontent.classList.remove("bg-white");
          navcontent.classList.add("bg-gray-100");
        }
      });
    </script>

    <script>
      /*Toggle dropdown list*/
      /*https://gist.github.com/slavapas/593e8e50cf4cc16ac972afcbad4f70c8*/

      var navMenuDiv = document.getElementById("nav-content");
      var navMenu = document.getElementById("nav-toggle");

      document.onclick = check;
      function check(e) {
        var target = (e && e.target) || (event && event.srcElement);

        //Nav Menu
        if (!checkParent(target, navMenuDiv)) {
          // click NOT on the menu
          if (checkParent(target, navMenu)) {
            // click on the link
            if (navMenuDiv.classList.contains("hidden")) {
              navMenuDiv.classList.remove("hidden");
            } else {
              navMenuDiv.classList.add("hidden");
            }
          } else {
            // click both outside link and outside menu, hide menu
            navMenuDiv.classList.add("hidden");
          }
        }
      }
      function checkParent(t, elm) {
        while (t.parentNode) {
          if (t == elm) {
            return true;
          }
          t = t.parentNode;
        }
        return false;
      }
    </script>
  

</body>

<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</html>